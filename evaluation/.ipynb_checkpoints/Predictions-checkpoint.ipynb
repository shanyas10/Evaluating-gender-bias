{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQdBlleYgrGk"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "from transformers import BartTokenizer, BartModel, BartConfig, BartForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "uczOhxwnszY_",
    "outputId": "b82945ec-4fb1-4d94-9a63-6b1c6fb4777f"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYJ-rgpsth8n"
   },
   "source": [
    "**Model loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dJGc-EoBiQZ"
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE    = 'bert-base-uncased'\n",
    "tokenizer_b     = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model_b = BertForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3)\n",
    "model_b.load_state_dict(torch.load('/content/drive/My Drive/Gender_Bias_NLI/Models/MNLI_BERT.bin', map_location=torch.device('cpu')))\n",
    "model_b = model_ba.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07pgQFcIkMGz"
   },
   "outputs": [],
   "source": [
    "\n",
    "MODEL_TYPE    = 'facebook/bart-base'\n",
    "tokenizer_ba     = BartTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model_ba = BartForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3)\n",
    "model_ba.load_state_dict(torch.load('/content/drive/My Drive/Gender Bias NLI Final/Models/MNLI_BART.bin', map_location=torch.device('cpu')))\n",
    "model_ba = model_ba.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV0b43E_qvLV"
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE    = 'roberta-base'\n",
    "tokenizer_r     = RobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model_r = RobertaForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3)\n",
    "model_r.load_state_dict(torch.load('/content/drive/My Drive/Gender_Bias_NLI/Models/MNLI_Roberta.bin', map_location=torch.device('cpu')))\n",
    "model_r = model_r.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr-tNoVHtlCm"
   },
   "source": [
    "**Evaluation set loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Amgfog6El7gr"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/Gender Bias NLI Final/Datasets/MNLI_Evaluation_Set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzNJbEMBxGwf"
   },
   "outputs": [],
   "source": [
    "def get_predictions(s1, s2, model, tokenizer, remove_token_type_ids):\n",
    "  encoded_seq= tokenizer.encode_plus(\n",
    "    s1,\n",
    "    s2,\n",
    "    max_length=180,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=True,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    "  )\n",
    "  input_ids = encoded_seq['input_ids'].to(device)\n",
    "  attention_mask = encoded_seq['attention_mask'].to(device)\n",
    "  token_type_ids= encoded_seq['token_type_ids'].to(device)\n",
    "\n",
    "  if remove_token_type_ids:\n",
    "      output = model(\n",
    "        input_ids,\n",
    "        attention_mask\n",
    "      )\n",
    "  else:\n",
    "      output = model(\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids\n",
    "      )\n",
    "\n",
    "  logit = output[0]\n",
    "  entail_contradiction_logits = logit[:,[0,2]]\n",
    "  probs = entail_contradiction_logits.softmax(dim=1)\n",
    "\n",
    "  entail_prob = probs[:,0].item()*100\n",
    "  cont_prob = probs[:,1].item()*100\n",
    "\n",
    "  entail_contradiction_logits = entail_contradiction_logits.detach().cpu().numpy()\n",
    "  prediction = np.argmax(entail_contradiction_logits, axis=1)\n",
    "\n",
    "\n",
    "  return prediction, entail_prob, cont_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGEd0f9J-5d9"
   },
   "outputs": [],
   "source": [
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "\n",
    "e_1 = []\n",
    "e_2 = []\n",
    "\n",
    "c_1 = []\n",
    "c_2 = []\n",
    "\n",
    "for i,r in df.iterrows(): \n",
    "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_f'], model_r, tokenizer_r, false) #Predictions for female hypothesis\n",
    "  predictions_1.append(prediction)\n",
    "  e_1.append(e)\n",
    "  c_1.append(c)\n",
    "\n",
    "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_m'], model_r, tokenizer_r, false) #Predictions for male hypothesis\n",
    "  # print(e,n,c)\n",
    "  predictions_2.append(prediction)\n",
    "  e_2.append(e)\n",
    "  c_2.append(c)\n",
    "  if np.sum(e_1)>np.sum(e_2):\n",
    "    print(np.sum(e_1)-np.sum(e_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOzGqzkCpuZy"
   },
   "outputs": [],
   "source": [
    "df['f_r_pred'] = predictions_1\n",
    "df['m_r_pred'] = predictions_2\n",
    "\n",
    "df['f_r_0'] = e_1  #entailment\n",
    "df['m_r_0'] = e_2\n",
    "\n",
    "df['f_r_1'] = c_1  #contradiction\n",
    "df['m_r_1'] = c_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe4O6edDqSha"
   },
   "outputs": [],
   "source": [
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "\n",
    "e_1 = []\n",
    "e_2 = []\n",
    "\n",
    "c_1 = []\n",
    "c_2 = []\n",
    "\n",
    "for i,r in df.iterrows(): \n",
    "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_f'], model_b, tokenizer_b, false) \n",
    "  predictions_1.append(prediction)\n",
    "  e_1.append(e)\n",
    "  c_1.append(c)\n",
    "\n",
    "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_m'], model_b, tokenizer_b, false)\n",
    "  # print(e,n,c)\n",
    "  predictions_2.append(prediction)\n",
    "  e_2.append(e)\n",
    "  c_2.append(c)\n",
    "  if np.sum(e_1)>np.sum(e_2):\n",
    "    print(np.sum(e_1)-np.sum(e_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYKyRGSq0EYJ"
   },
   "outputs": [],
   "source": [
    "df['f_b_pred'] = predictions_1\n",
    "df['m_b_pred'] = predictions_2\n",
    "\n",
    "df['f_b_0'] = e_1 #entailment\n",
    "df['m_b_0'] = e_2\n",
    "\n",
    "df['f_b_1'] = c_1 #contradiction\n",
    "df['m_b_1'] = c_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1da1I7VVnou"
   },
   "outputs": [],
   "source": [
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "\n",
    "e_1 = []\n",
    "e_2 = []\n",
    "\n",
    "c_1 = []\n",
    "c_2 = []\n",
    "\n",
    "for i,r in df.iterrows(): \n",
    "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_f'], model_ba, tokenizer_ba, true) #BART doesn't support token type ids\n",
    "  predictions_1.append(prediction)\n",
    "  e_1.append(e)\n",
    "  c_1.append(c)\n",
    "\n",
    "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_m'], model_ba, tokenizer_ba, true)\n",
    "  # print(e,n,c)\n",
    "  predictions_2.append(prediction)\n",
    "  e_2.append(e)\n",
    "  c_2.append(c)\n",
    "  if np.sum(e_1)>np.sum(e_2):\n",
    "    print(np.sum(e_1)-np.sum(e_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD1e6Xa1V_7H"
   },
   "outputs": [],
   "source": [
    "df['f_ba_pred'] = predictions_1\n",
    "df['m_ba_pred'] = predictions_2\n",
    "\n",
    "df['f_ba_0'] = e_1\n",
    "df['m_ba_0'] = e_2\n",
    "\n",
    "df['f_ba_1'] = c_1\n",
    "df['m_ba_1'] = c_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Se_qWJt0IOe"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/My Drive/Gender Bias NLI Final/Results/MNLI_Predictions_Final.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Predictions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
